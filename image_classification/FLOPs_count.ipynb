{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    "Plese install the following packages first:\n",
    "```\n",
    "pip install timm fvcore==0.1.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numbers import Number\n",
    "from typing import Any, Callable, List, Optional, Union\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "import torch \n",
    "from spanet import spanet_small, spanet_medium, spanet_mediumX, spanet_base, spanet_baseX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfft_flop_jit(inputs: List[Any], outputs: List[Any]) -> Number:\n",
    "    \"\"\"\n",
    "    Count flops for the rfft/rfftn operator.\n",
    "    \"\"\"\n",
    "    input_shape = inputs[0].type().sizes()\n",
    "    B, H, W, C = input_shape\n",
    "    N = H * W\n",
    "    flops = N * C * np.ceil(np.log2(N))\n",
    "    return flops\n",
    "\n",
    "def calc_ofcnet_flops(model, img_size=224, show_details=False):\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(1, 3, img_size, img_size).cuda()\n",
    "        fca1 = FlopCountAnalysis(model, x)\n",
    "        handlers = {\n",
    "            'aten::fft_rfft2': rfft_flop_jit,\n",
    "            'aten::fft_irfft2': rfft_flop_jit,\n",
    "        }\n",
    "        fca1.set_op_handle(**handlers) # 이건 뭔데 추가된 걸까? \n",
    "        flops1 = fca1.total()\n",
    "        if show_details:\n",
    "            print(fca1.by_module())\n",
    "        print(\"#### GFLOPs: {}\".format(flops1 / 1e9))\n",
    "    return flops1 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spanet_baseX().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = [224, 224]    \n",
    "dummy_input = torch.rand(1, 3, *image_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Simple way == # \n",
    "# Please note that FLOP here actually means MAC.\n",
    "flop = FlopCountAnalysis(model, dummy_input)\n",
    "print(flop_count_table(flop, max_depth=4))\n",
    "print('MACs (G):', flop.total()/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT LOGGERS\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "#GPU-WARM-UP\n",
    "for _ in range(10):\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "\n",
    "print(f\"mean: {mean_syn:.03} ms\")\n",
    "print(f\"std: {std_syn}\")\n",
    "\n",
    "# MEASURE MACs \n",
    "# Please note that FLOPs here actually means MACs.    \n",
    "with torch.no_grad():\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    handlers = {\n",
    "                'aten::fft_rfft2': rfft_flop_jit,\n",
    "                'aten::fft_irfft2': rfft_flop_jit,\n",
    "            }\n",
    "    flops.set_op_handle(**handlers) \n",
    "        \n",
    "    if False:\n",
    "        # show_details\n",
    "        print(flops.by_module())\n",
    "        \n",
    "    print(flop_count_table(flops))\n",
    "    print(f'MACs (G): {flops.total()/1e9:0.3f}G' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('shiformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd85fdd0f103ea7eaab582a753ef3647c7e4c49ee7c98ba5d5dec099b22484f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
